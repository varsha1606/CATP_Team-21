{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c5ab691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.29773235321045\n",
      "Epoch 2/10, Loss: 6.782962322235107\n",
      "Epoch 3/10, Loss: 6.601043701171875\n",
      "Epoch 4/10, Loss: 4.7232584953308105\n",
      "Epoch 5/10, Loss: 5.592641830444336\n",
      "Epoch 6/10, Loss: 4.45326042175293\n",
      "Epoch 7/10, Loss: 4.140463352203369\n",
      "Epoch 8/10, Loss: 2.6678214073181152\n",
      "Epoch 9/10, Loss: 2.4368321895599365\n",
      "Epoch 10/10, Loss: 3.4015085697174072\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalnum()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load and preprocess text files\n",
    "def load_and_preprocess(file_paths):\n",
    "    tokens = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        tokens.extend(preprocess_text(text))\n",
    "    return tokens\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "def train_and_save_model(file_paths, model_path):\n",
    "    tokens = load_and_preprocess(file_paths)\n",
    "    vocab = list(set(tokens))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    sequence_length = 5\n",
    "    \n",
    "    for i in range(len(tokens) - sequence_length):\n",
    "        seq = tokens[i:i + sequence_length]\n",
    "        target = tokens[i + sequence_length]\n",
    "        sequences.append([word_to_idx[word] for word in seq])\n",
    "        targets.append(word_to_idx[target])\n",
    "\n",
    "    dataset = TextDataset(sequences, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 128\n",
    "    output_dim = vocab_size\n",
    "    \n",
    "    model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Save the model and dictionaries\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'word_to_idx': word_to_idx,\n",
    "        'idx_to_word': idx_to_word\n",
    "    }, model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = [\"C:/Users/varsh/Downloads/Sherlock Holmes.txt\"]\n",
    "    model_path = \"lstm_model.pth\"\n",
    "    train_and_save_model(file_paths, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24414c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word or a sentence: good\n",
      "Input tokens: ['good']\n",
      "Input sequence (token indices): [2638]\n",
      "Model output: tensor([[-0.6026,  3.0582, -3.5429,  ..., -2.1907,  0.2534, -1.1391]])\n",
      "LSTM Predictions: [('enough', 0.0551033690571785), ('deal', 0.03613436594605446), ('heaven', 0.015592057257890701), ('made', 0.014579476788640022), ('sen', 0.013655569404363632), ('cri', 0.008337687700986862), ('chanc', 0.00746711902320385), ('come', 0.007198211271315813), ('word', 0.007075964007526636), ('articl', 0.0069364639930427074)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "def load_model(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    word_to_idx = checkpoint['word_to_idx']\n",
    "    idx_to_word = checkpoint['idx_to_word']\n",
    "    \n",
    "    vocab_size = len(word_to_idx)\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 128\n",
    "    output_dim = vocab_size\n",
    "    \n",
    "    model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, word_to_idx, idx_to_word\n",
    "\n",
    "def predict_next_word_with_probs(model, word_to_idx, idx_to_word, input_sentence, sequence_length=5, top_n=10):\n",
    "    input_tokens = preprocess_text(input_sentence)\n",
    "    print(f\"Input tokens: {input_tokens}\")  # Debugging: print input tokens\n",
    "    \n",
    "    if not input_tokens:\n",
    "        return \"No valid input tokens found.\"\n",
    "    \n",
    "    unk_token = '<UNK>'\n",
    "    if unk_token not in word_to_idx:\n",
    "        word_to_idx[unk_token] = len(word_to_idx)\n",
    "        idx_to_word[len(idx_to_word)] = unk_token\n",
    "    \n",
    "    input_sequence = [word_to_idx.get(token, word_to_idx[unk_token]) for token in input_tokens[-sequence_length:]]\n",
    "    print(f\"Input sequence (token indices): {input_sequence}\")  # Debugging: print input sequence\n",
    "    \n",
    "    if len(input_sequence) == 0:\n",
    "        return \"Input sequence is empty after preprocessing.\"\n",
    "    \n",
    "    # Ensure all token indices are within the range of the vocabulary size\n",
    "    vocab_size = model.embedding.num_embeddings\n",
    "    input_sequence = [idx if idx < vocab_size else word_to_idx[unk_token] for idx in input_sequence]\n",
    "    \n",
    "    input_sequence = torch.tensor([input_sequence], dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_sequence)\n",
    "        print(f\"Model output: {output}\")  # Debugging: print model output\n",
    "        probabilities = torch.softmax(output, dim=1).squeeze().tolist()\n",
    "        \n",
    "        predictions = {idx_to_word[idx]: prob for idx, prob in enumerate(probabilities)}\n",
    "        \n",
    "        # Sort predictions by probability\n",
    "        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return sorted_predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"lstm_model.pth\"\n",
    "    model, word_to_idx, idx_to_word = load_model(model_path)\n",
    "    \n",
    "    input_sentence = input(\"Enter a word or a sentence: \")\n",
    "    \n",
    "    predictions_lstm = predict_next_word_with_probs(model, word_to_idx, idx_to_word, input_sentence)\n",
    "    \n",
    "    print(\"LSTM Predictions:\", predictions_lstm)\n",
    "    \n",
    "    # Save predictions to a file\n",
    "    with open('lstm_predictions.json', 'w') as f:\n",
    "        json.dump(predictions_lstm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23517d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
